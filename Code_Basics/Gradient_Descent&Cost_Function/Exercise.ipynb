{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743e0849-d85a-4b88-b98c-23e5b15ecad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f15c1b-fdd1-49d1-b14b-73d4bc59cecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>math</th>\n",
       "      <th>cs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>david</td>\n",
       "      <td>92</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laura</td>\n",
       "      <td>56</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sanjay</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wei</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jeff</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aamir</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>venkat</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>virat</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arthur</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paul</td>\n",
       "      <td>67</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  math  cs\n",
       "0   david    92  98\n",
       "1   laura    56  68\n",
       "2  sanjay    88  81\n",
       "3     wei    70  80\n",
       "4    jeff    80  83\n",
       "5   aamir    49  52\n",
       "6  venkat    65  66\n",
       "7   virat    35  30\n",
       "8  arthur    66  68\n",
       "9    paul    67  73"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"test_scores.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9707a2-0d3f-427f-9e83-c74172c843fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array(df.math)\n",
    "y= np.array(df.cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ffa8824-ab97-4c59-a4ed-1e1465e98961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y):\n",
    "    m_curr = b_curr = 0\n",
    "    learning_rate = 0.00014\n",
    "    threshold=1e-20\n",
    "    iterations = 100\n",
    "    n = len(x)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # prediction\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        \n",
    "        # calculating gradient\n",
    "        md = -(2/n) * sum(x * (y - y_predicted))\n",
    "        bd = -(2/n) * sum(y - y_predicted)\n",
    "\n",
    "        # updating parameter\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        \n",
    "        # cost function\n",
    "        cost = (1/n) * sum([val**2 for val in (y - y_predicted)])\n",
    "\n",
    "        # # check for covergence\n",
    "        # cost_old = np.mean((y - y_predicted) ** 2)\n",
    "        # cost_new = np.mean((y - (m_curr * x + b_curr)) ** 2)\n",
    "\n",
    "        # if math.isclose(cost_old, cost_new, rel_tol=threshold):\n",
    "        #     break\n",
    "\n",
    "        print(\"m {}, b {}, cost {}, iterations {}\".format(m_curr, b_curr, cost, i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce584c30-b318-46cc-90f0-424bc70fd9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 1.384852, b 0.019572, cost 5199.1, iterations 0\n",
      "m 0.9336889022720001, b 0.013236248031999999, cost 580.2428445154782, iterations 1\n",
      "m 1.0806700177492279, b 0.01534082465445555, cost 90.0196550053916, iterations 2\n",
      "m 1.0327853610379343, b 0.014695677211570743, cost 37.98972811327158, iterations 3\n",
      "m 1.0483848413283663, b 0.014946345029097979, cost 32.46751708720706, iterations 4\n",
      "m 1.043302210013366, b 0.014905169980284069, cost 31.881410332189844, iterations 5\n",
      "m 1.044957478297138, b 0.014959071996599592, cost 31.81919783705809, iterations 6\n",
      "m 1.0444176471820912, b 0.014981998782370875, cost 31.812588947789234, iterations 7\n",
      "m 1.0445929441840602, b 0.015015016149817977, cost 31.811881556802305, iterations 8\n",
      "m 1.0445352639471386, b 0.015044745517277365, cost 31.81180052252267, iterations 9\n",
      "m 1.0445534839597723, b 0.015075545411665248, cost 31.81178596690923, iterations 10\n",
      "m 1.0445469769000353, b 0.015105995894966402, cost 31.81177846721171, iterations 11\n",
      "m 1.044548525532281, b 0.015136559560177551, cost 31.811771716586364, iterations 12\n",
      "m 1.0445474497587184, b 0.015167085701944917, cost 31.81176504565532, iterations 13\n",
      "m 1.0445472289835727, b 0.015197623417661303, cost 31.811758383374013, iterations 14\n",
      "m 1.0445467296728634, b 0.015228156712195615, cost 31.811751722202132, iterations 15\n",
      "m 1.0445463211137445, b 0.015258690796514962, cost 31.81174506133938, iterations 16\n",
      "m 1.044545882998396, b 0.015289223972980461, cost 31.811738400700786, iterations 17\n",
      "m 1.0445454545211714, b 0.015319756794666026, cost 31.811731740277335, iterations 18\n",
      "m 1.0445450229131807, b 0.015350289081399529, cost 31.811725080068086, iterations 19\n",
      "m 1.044544592334323, b 0.015380820891888605, cost 31.811718420072893, iterations 20\n",
      "m 1.0445441614293691, b 0.015411352207017696, cost 31.811711760291765, iterations 21\n",
      "m 1.0445437306398315, b 0.015441883033024809, cost 31.81170510072468, iterations 22\n",
      "m 1.0445432998218724, b 0.015472413367888155, cost 31.811698441371647, iterations 23\n",
      "m 1.044542869022351, b 0.015502943212276844, cost 31.811691782232646, iterations 24\n",
      "m 1.0445424382260018, b 0.015533472565983352, cost 31.81168512330767, iterations 25\n",
      "m 1.0445420074377978, b 0.01556400142908574, cost 31.81167846459672, iterations 26\n",
      "m 1.0445415766561186, b 0.015594529801569028, cost 31.8116718060998, iterations 27\n",
      "m 1.0445411458814917, b 0.015625057683448546, cost 31.81166514781687, iterations 28\n",
      "m 1.0445407151137456, b 0.015655585074729757, cost 31.811658489747956, iterations 29\n",
      "m 1.0445402843529359, b 0.015686111975421337, cost 31.81165183189303, iterations 30\n",
      "m 1.044539853599044, b 0.015716638385530904, cost 31.811645174252103, iterations 31\n",
      "m 1.0445394228520763, b 0.01574716430506644, cost 31.811638516825152, iterations 32\n",
      "m 1.04453899211203, b 0.015777689734035785, cost 31.81163185961217, iterations 33\n",
      "m 1.0445385613789067, b 0.015808214672446846, cost 31.81162520261313, iterations 34\n",
      "m 1.0445381306527048, b 0.01583873912030749, cost 31.811618545828082, iterations 35\n",
      "m 1.0445376999334253, b 0.015869263077625612, cost 31.811611889257012, iterations 36\n",
      "m 1.0445372692210677, b 0.015899786544409093, cost 31.811605232899822, iterations 37\n",
      "m 1.0445368385156315, b 0.01593030952066581, cost 31.811598576756637, iterations 38\n",
      "m 1.044536407817117, b 0.01596083200640365, cost 31.811591920827354, iterations 39\n",
      "m 1.0445359771255243, b 0.0159913540016305, cost 31.81158526511198, iterations 40\n",
      "m 1.0445355464408528, b 0.016021875506354238, cost 31.811578609610564, iterations 41\n",
      "m 1.0445351157631027, b 0.016052396520582747, cost 31.811571954323018, iterations 42\n",
      "m 1.0445346850922739, b 0.01608291704432391, cost 31.811565299249402, iterations 43\n",
      "m 1.044534254428366, b 0.01611343707758561, cost 31.81155864438971, iterations 44\n",
      "m 1.044533823771379, b 0.016143956620375727, cost 31.811551989743858, iterations 45\n",
      "m 1.044533393121313, b 0.016174475672702148, cost 31.811545335311934, iterations 46\n",
      "m 1.044532962478168, b 0.016204994234572754, cost 31.811538681093857, iterations 47\n",
      "m 1.0445325318419434, b 0.01623551230599542, cost 31.811532027089687, iterations 48\n",
      "m 1.0445321012126392, b 0.016266029886978034, cost 31.811525373299332, iterations 49\n",
      "m 1.0445316705902556, b 0.016296546977528477, cost 31.811518719722887, iterations 50\n",
      "m 1.0445312399747924, b 0.01632706357765463, cost 31.811512066360258, iterations 51\n",
      "m 1.0445308093662493, b 0.01635757968736437, cost 31.811505413211485, iterations 52\n",
      "m 1.044530378764626, b 0.01638809530666558, cost 31.81149876027654, iterations 53\n",
      "m 1.044529948169923, b 0.01641861043556615, cost 31.811492107555434, iterations 54\n",
      "m 1.0445295175821399, b 0.01644912507407395, cost 31.81148545504815, iterations 55\n",
      "m 1.0445290870012764, b 0.016479639222196866, cost 31.811478802754664, iterations 56\n",
      "m 1.0445286564273326, b 0.016510152879942775, cost 31.811472150675016, iterations 57\n",
      "m 1.0445282258603081, b 0.01654066604731956, cost 31.81146549880919, iterations 58\n",
      "m 1.0445277953002035, b 0.016571178724335108, cost 31.811458847157077, iterations 59\n",
      "m 1.044527364747018, b 0.01660169091099729, cost 31.811452195718847, iterations 60\n",
      "m 1.0445269342007515, b 0.016632202607313986, cost 31.811445544494326, iterations 61\n",
      "m 1.0445265036614042, b 0.01666271381329308, cost 31.81143889348365, iterations 62\n",
      "m 1.0445260731289758, b 0.016693224528942455, cost 31.811432242686678, iterations 63\n",
      "m 1.0445256426034666, b 0.016723734754269987, cost 31.811425592103504, iterations 64\n",
      "m 1.0445252120848758, b 0.016754244489283553, cost 31.811418941734043, iterations 65\n",
      "m 1.0445247815732037, b 0.016784753733991038, cost 31.81141229157839, iterations 66\n",
      "m 1.0445243510684503, b 0.016815262488400318, cost 31.811405641636412, iterations 67\n",
      "m 1.044523920570615, b 0.01684577075251927, cost 31.811398991908217, iterations 68\n",
      "m 1.0445234900796982, b 0.01687627852635578, cost 31.81139234239376, iterations 69\n",
      "m 1.0445230595956998, b 0.016906785809917724, cost 31.811385693092987, iterations 70\n",
      "m 1.0445226291186192, b 0.01693729260321298, cost 31.811379044005914, iterations 71\n",
      "m 1.0445221986484565, b 0.016967798906249425, cost 31.811372395132583, iterations 72\n",
      "m 1.044521768185212, b 0.016998304719034945, cost 31.811365746472934, iterations 73\n",
      "m 1.044521337728885, b 0.01702881004157741, cost 31.811359098026976, iterations 74\n",
      "m 1.0445209072794757, b 0.017059314873884703, cost 31.811352449794715, iterations 75\n",
      "m 1.0445204768369838, b 0.0170898192159647, cost 31.811345801776127, iterations 76\n",
      "m 1.0445200464014095, b 0.017120323067825283, cost 31.811339153971222, iterations 77\n",
      "m 1.0445196159727526, b 0.01715082642947433, cost 31.811332506379962, iterations 78\n",
      "m 1.0445191855510128, b 0.01718132930091971, cost 31.81132585900238, iterations 79\n",
      "m 1.0445187551361899, b 0.01721183168216931, cost 31.811319211838438, iterations 80\n",
      "m 1.044518324728284, b 0.017242333573231006, cost 31.81131256488816, iterations 81\n",
      "m 1.0445178943272952, b 0.017272834974112677, cost 31.8113059181515, iterations 82\n",
      "m 1.044517463933223, b 0.017303335884822198, cost 31.811299271628457, iterations 83\n",
      "m 1.0445170335460676, b 0.017333836305367445, cost 31.811292625319073, iterations 84\n",
      "m 1.0445166031658284, b 0.017364336235756294, cost 31.81128597922327, iterations 85\n",
      "m 1.0445161727925059, b 0.017394835675996628, cost 31.811279333341076, iterations 86\n",
      "m 1.0445157424260996, b 0.017425334626096318, cost 31.811272687672524, iterations 87\n",
      "m 1.0445153120666095, b 0.017455833086063244, cost 31.811266042217557, iterations 88\n",
      "m 1.0445148817140355, b 0.01748633105590528, cost 31.81125939697616, iterations 89\n",
      "m 1.0445144513683775, b 0.017516828535630306, cost 31.81125275194833, iterations 90\n",
      "m 1.0445140210296353, b 0.017547325525246196, cost 31.811246107134124, iterations 91\n",
      "m 1.044513590697809, b 0.01757782202476083, cost 31.81123946253348, iterations 92\n",
      "m 1.044513160372898, b 0.01760831803418208, cost 31.811232818146394, iterations 93\n",
      "m 1.0445127300549029, b 0.01763881355351782, cost 31.811226173972827, iterations 94\n",
      "m 1.044512299743823, b 0.017669308582775933, cost 31.811219530012842, iterations 95\n",
      "m 1.0445118694396585, b 0.01769980312196429, cost 31.811212886266404, iterations 96\n",
      "m 1.044511439142409, b 0.017730297171090766, cost 31.811206242733473, iterations 97\n",
      "m 1.044511008852075, b 0.01776079073016324, cost 31.811199599414067, iterations 98\n",
      "m 1.0445105785686555, b 0.017791283799189584, cost 31.81119295630822, iterations 99\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10090055-8bc5-46cc-bb75-ae9bb19246c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn Linear Regression Results:\n",
      "Slope (m): 1.0177362378569328\n",
      "Intercept (b): 1.9152193111569034\n"
     ]
    }
   ],
   "source": [
    "x_reshaped = x.reshape(-1, 1)\n",
    "model = LinearRegression()\n",
    "model.fit(x_reshaped, y)\n",
    "m_sklearn = model.coef_[0]\n",
    "b_sklearn = model.intercept_\n",
    "\n",
    "print(\"\\nSklearn Linear Regression Results:\")\n",
    "print(f\"Slope (m): {m_sklearn}\")\n",
    "print(f\"Intercept (b): {b_sklearn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
